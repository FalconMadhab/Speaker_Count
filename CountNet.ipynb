{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchaudio_contrib import Melspectrogram\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import datetime\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import librosa\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "\n",
    "from net_config.audio import MelspectrogramStretch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "from visualization import WriterTensorboardX\n",
    "\n",
    "from transforms import AudioTransforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, errno\n",
    "\n",
    "\n",
    "def mkdir_p(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc:  # Python >2.5\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "\n",
    "def setup_logging(logging_path='logs'):\n",
    "\n",
    "    log_path = os.path.join(os.getcwd(),logging_path)\n",
    "    mkdir_p(log_path)\n",
    "\n",
    "    check_names = lambda y: y if y.isdigit() else -1\n",
    "    get_ind = lambda x: int(check_names(x.split('_')[1]))\n",
    "    \n",
    "    run_counter = max(map(get_ind, os.listdir(log_path)), default=-1) + 1\n",
    "\n",
    "    run_path = os.path.join(log_path, 'run_%s'%run_counter)\n",
    "    mkdir_p(run_path)\n",
    "\n",
    "    print('Logging set up, to monitor training run:\\n'\n",
    "        '\\t\\'tensorboard --logdir=%s\\'\\n'%run_path)\n",
    "\n",
    "    return run_path\n",
    "\n",
    "\n",
    "def list_dir(path):\n",
    "    filter_dir = lambda x: os.path.isdir(os.path.join(path,x))\n",
    "    filter_file = lambda x: os.path.isfile(os.path.join(path,x)) and not x.startswith('.') \\\n",
    "    and not x.split('.')[-1] in ['pyc', 'py','txt']\n",
    "\n",
    "    ret = [n for n in os.listdir(path) if filter_dir(n) or filter_file(n)]\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundSet(Dataset):\n",
    "    def __init__(self, transform=None, mode=\"train\"):\n",
    "        # setting directories for data\n",
    "        self.mode = mode\n",
    "        self.all_files = list(Path(\"data/processed\").glob('**/*.wav'))\n",
    "        if self.mode is \"train\":\n",
    "            self.files = [file for i, file in enumerate(self.all_files) if i%5 !=0]\n",
    "        elif self.mode is \"test\":\n",
    "            self.files = [file for i, file in enumerate(self.all_files) if i%5 ==0]\n",
    "        with open(str(Path(\"data/processed\") / 'labels.json')) as f:\n",
    "            self.classes = json.load(f)\n",
    "        # dict for mapping class names into indices. can be obtained by \n",
    "        # {cls_name:i for i, cls_name in enumerate(csv_file[\"label\"].unique())}\n",
    "        #self.classes = {'Acoustic_guitar': 38, 'Applause': 37, 'Bark': 19, 'Bass_drum': 21, 'Burping_or_eructation': 28, 'Bus': 22, 'Cello': 4, 'Chime': 20, 'Clarinet': 7,'Computer_keyboard': 8, 'Cough': 17, 'Cowbell': 33, 'Double_bass': 29, 'Drawer_open_or_close': 36, 'Electric_piano': 34, 'Fart': 14, 'Finger_snapping': 40, 'Fireworks': 31, 'Flute': 16, 'Glockenspiel': 3, 'Gong': 26, 'Gunshot_or_gunfire': 6, 'Harmonica': 25, 'Hi-hat': 0, 'Keys_jangling': 9, 'Knock': 5, 'Laughter': 12, 'Meow': 35, 'Microwave_oven': 27, 'Oboe': 15, 'Saxophone': 1, 'Scissors': 24, 'Shatter': 30, 'Snare_drum': 10, 'Squeak': 23, 'Tambourine': 32, 'Tearing': 13, 'Telephone': 18, 'Trumpet': 2, 'Violin_or_fiddle': 39,  'Writing': 11}\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.files[idx]\n",
    "        \n",
    "        data, sr = librosa.load(str(filename))\n",
    "        data = data.reshape(-1, 1)   \n",
    "        \n",
    "        if self.transform is not None:\n",
    "            data = self.transform.apply(data)\n",
    "\n",
    "        #if self.mode is \"train\":\n",
    "        label = self.classes[filename.stem]\n",
    "        return  data, sr, label\n",
    "\n",
    "#         elif self.mode is \"test\":\n",
    "#             return torch.from_numpy(data).float(), sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(batch):\n",
    "        sort_ind = 0\n",
    "        sorted_batch = sorted(batch, key=lambda x: x[0].size(sort_ind), reverse=True)\n",
    "        seqs, srs, labels = zip(*sorted_batch)\n",
    "        \n",
    "        lengths, srs, labels = map(torch.LongTensor, [[x.size(sort_ind) for x in seqs], srs, labels])\n",
    "\n",
    "        # seqs_pad -> (batch, time, channel) \n",
    "        seqs_pad = torch.nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0)\n",
    "        #seqs_pad = seqs_pad_t.transpose(0,1)\n",
    "        return seqs_pad, lengths, srs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "dataloader = DataLoader(\n",
    "        SoundSet(mode=\"train\", transform=AudioTransforms(\"train\", {\"noise\":[0.3, 0.001], \"crop\":[0.4, 0.25]})),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, \n",
    "        num_workers=0,\n",
    "        collate_fn=pad_seq)\n",
    "test_dataloader = DataLoader(\n",
    "        SoundSet(mode=\"test\", transform=AudioTransforms(\"train\", {\"noise\":[0.3, 0.001], \"crop\":[0.4, 0.25]})),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, \n",
    "        num_workers=0,\n",
    "        collate_fn=pad_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioCRNN(nn.Module):\n",
    "    def __init__(self, classes, state_dict=None):\n",
    "        super(AudioCRNN, self).__init__()\n",
    "        \n",
    "        in_chan = 1\n",
    "\n",
    "        self.classes = classes\n",
    "        self.lstm_units = 64\n",
    "        self.lstm_layers = 2\n",
    "        self.spec = MelspectrogramStretch(hop_length=None, \n",
    "                                num_mels=128, \n",
    "                                fft_length=2048, \n",
    "                                norm='whiten', \n",
    "                                stretch_param=[0.4, 0.4])\n",
    "\n",
    "        # shape -> (channel, freq, token_time)\n",
    "        self.net = nn.ModuleDict({\n",
    "            'convs' : nn.Sequential(\n",
    "                nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=[0, 0]),\n",
    "                nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ELU(alpha=1.0),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False),\n",
    "                nn.Dropout(p=0.1),\n",
    "                nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=[0, 0]),\n",
    "                nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ELU(alpha=1.0),\n",
    "                nn.MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False),\n",
    "                nn.Dropout(p=0.1),\n",
    "                nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=[0, 0]),\n",
    "                nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.ELU(alpha=1.0),\n",
    "                nn.MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False),\n",
    "                nn.Dropout(p=0.1)\n",
    "            ),\n",
    "            'recur' : nn.LSTM(128, 64, num_layers=2),\n",
    "            'dense' : nn.Sequential(\n",
    "                nn.Dropout(p=0.3),\n",
    "                nn.BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.Linear(in_features=64, out_features=4, bias=True)\n",
    "            )\n",
    "        })\n",
    "        #self.net = parse_cfg(config['cfg'], in_shape=[in_chan, self.spec.num_mels, 400])\n",
    "\n",
    "    def _many_to_one(self, t, lengths):\n",
    "        return t[torch.arange(t.size(0)), lengths - 1]\n",
    "\n",
    "    def modify_lengths(self, lengths):\n",
    "        def safe_param(elem):\n",
    "            return elem if isinstance(elem, int) else elem[0]\n",
    "        \n",
    "        for name, layer in self.net['convs'].named_children():\n",
    "            #if name.startswith(('conv2d','maxpool2d')):\n",
    "            if isinstance(layer, (nn.Conv2d, nn.MaxPool2d)):\n",
    "                p, k, s = map(safe_param, [layer.padding, layer.kernel_size,layer.stride]) \n",
    "                lengths = (lengths + 2*p - k)//s + 1\n",
    "\n",
    "        return torch.where(lengths > 0, lengths, torch.tensor(1, device=lengths.device))\n",
    "\n",
    "    def forward(self, batch):    \n",
    "        # x-> (batch, time, channel)\n",
    "        #print(batch)\n",
    "        x, lengths, _ = batch # unpacking seqs, lengths and srs\n",
    "\n",
    "        # x-> (batch, channel, time)\n",
    "        xt = x.float().transpose(1,2)\n",
    "        # xt -> (batch, channel, freq, time)\n",
    "        xt, lengths = self.spec(xt, lengths)                \n",
    "\n",
    "        # (batch, channel, freq, time)\n",
    "        xt = self.net['convs'](xt)\n",
    "        lengths = self.modify_lengths(lengths)\n",
    "\n",
    "        # xt -> (batch, time, freq, channel)\n",
    "        x = xt.transpose(1, -1)\n",
    "\n",
    "        # xt -> (batch, time, channel*freq)\n",
    "        batch, time = x.size()[:2]\n",
    "        x = x.reshape(batch, time, -1)\n",
    "        x_pack = torch.nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True)\n",
    "    \n",
    "        # x -> (batch, time, lstm_out)\n",
    "        x_pack, hidden = self.net['recur'](x_pack)\n",
    "        x, _ = torch.nn.utils.rnn.pad_packed_sequence(x_pack, batch_first=True)\n",
    "        \n",
    "        # (batch, lstm_out)\n",
    "        x = self._many_to_one(x, lengths)\n",
    "        # (batch, classes)\n",
    "        x = self.net['dense'](x)\n",
    "\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            out_raw = self.forward( x )\n",
    "            out = torch.exp(out_raw)\n",
    "            max_ind = out.argmax().item()        \n",
    "            return self.classes[max_ind], out[:,max_ind].item()\n",
    "\n",
    "\n",
    "class AudioCNN(AudioCRNN):\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, _, _ = batch\n",
    "        # x-> (batch, channel, time)\n",
    "        x = x.float().transpose(1,2)\n",
    "        # x -> (batch, channel, freq, time)\n",
    "        x = self.spec(x)                \n",
    "\n",
    "        # (batch, channel, freq, time)\n",
    "        x = self.net['convs'](x)\n",
    "\n",
    "        # x -> (batch, time*freq*channel)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # (batch, classes)\n",
    "        x = self.net['dense'](x)\n",
    "\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class AudioRNN(AudioCRNN):\n",
    "\n",
    "    def forward(self, batch):    \n",
    "        # x-> (batch, time, channel)\n",
    "        x, lengths, _ = batch # unpacking seqs, lengths and srs\n",
    "\n",
    "        # x-> (batch, channel, time)\n",
    "        x = x.float().transpose(1,2)\n",
    "        # x -> (batch, channel, freq, time)\n",
    "        x, lengths = self.spec(x, lengths)                \n",
    "\n",
    "        # x -> (batch, time, freq, channel)\n",
    "        x = x.transpose(1, -1)\n",
    "\n",
    "        # x -> (batch, time, channel*freq)\n",
    "        batch, time = x.size()[:2]\n",
    "        x = x.reshape(batch, time, -1)\n",
    "        x_pack = torch.nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True)\n",
    "    \n",
    "        # x -> (batch, time, lstm_out)\n",
    "        x_pack, hidden = self.net['recur'](x_pack)\n",
    "        x, _ = torch.nn.utils.rnn.pad_packed_sequence(x_pack, batch_first=True)\n",
    "        \n",
    "        # (batch, lstm_out)\n",
    "        x = self._many_to_one(x, lengths)\n",
    "        # (batch, classes)\n",
    "        x = self.net['dense'](x)\n",
    "\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AudioCRNN(classes=[0, 1, 2, 3]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioCRNN(\n",
       "  (spec): MelspectrogramStretch(num_mels=128, fft_length=2048, norm=spec_whiten, stretch_param=[0.4, 0.4])\n",
       "  (net): ModuleDict(\n",
       "    (convs): Sequential(\n",
       "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=[0, 0])\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ELU(alpha=1.0)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "      (4): Dropout(p=0.1)\n",
       "      (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=[0, 0])\n",
       "      (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (7): ELU(alpha=1.0)\n",
       "      (8): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "      (9): Dropout(p=0.1)\n",
       "      (10): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=[0, 0])\n",
       "      (11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (12): ELU(alpha=1.0)\n",
       "      (13): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "      (14): Dropout(p=0.1)\n",
       "    )\n",
       "    (dense): Sequential(\n",
       "      (0): Dropout(p=0.3)\n",
       "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Linear(in_features=64, out_features=4, bias=True)\n",
       "    )\n",
       "    (recur): LSTM(128, 64, num_layers=2)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(output, target, metrics):\n",
    "    acc_metrics = np.zeros(len(metrics))\n",
    "    for i, metric in enumerate(metrics):\n",
    "        #import pdb;pdb.set_trace()\n",
    "        acc_metrics[i] += metric(output, target)\n",
    "\n",
    "        writer.add_scalar(\"%s\"%metric.__name__, acc_metrics[i])\n",
    "        writer.add_scalar('{}'.format(metric.__name__), acc_metrics[i])\n",
    "    return acc_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, percent=0.1):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        assert output.shape[0] == len(target)\n",
    "        preds = torch.argmax(output,dim=1)\n",
    "        tp = 0\n",
    "        tp = torch.sum(preds == target).item()\n",
    "\n",
    "    return tp / len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.002, weight_decay=0.01, amsgrad=True)\n",
    "\n",
    "lr_scheduler = StepLR(optimizer, step_size=10, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, save_best=False):\n",
    "    \"\"\"\n",
    "    Saving checkpoints\n",
    "\n",
    "    :param epoch: current epoch number\n",
    "    :param log: logging information of the epoch\n",
    "    :param save_best: if True, rename the saved checkpoint to 'model_best.pth'\n",
    "    \"\"\"\n",
    "    arch = type(model).__name__\n",
    "    state = {\n",
    "        'arch': arch,\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'monitor_best': mnt_best,\n",
    "        'classes':model.classes\n",
    "    }\n",
    "\n",
    "    filename = os.path.join(checkpoint_dir, 'checkpoint-current.pth')\n",
    "    #filename = os.path.join(self.checkpoint_dir, 'checkpoint-epoch{}.pth'.format(epoch))\n",
    "    torch.save(state, filename)\n",
    "    logger.info(\"Saving checkpoint: {} ...\".format(filename))\n",
    "    if save_best:\n",
    "        best_path = os.path.join(checkpoint_dir, 'model_best.pth')\n",
    "        torch.save(state, best_path)\n",
    "        logger.info(\"Saving current best: {} ...\".format('model_best.pth'))\n",
    "        logger.info(\"[IMPROVED]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(epoch):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    total_val_metrics = np.zeros(len(metrics))\n",
    "\n",
    "\n",
    "    writer.set_step(epoch, 'valid')        \n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch_idx, batch in enumerate(test_dataloader):\n",
    "            batch = [b.to(\"cuda\") for b in batch]\n",
    "            data, target = batch[:-1], batch[-1]\n",
    "            data = data if len(data) > 1 else data[0] \n",
    "\n",
    "            output = model(data)\n",
    "            loss = loss_fn.forward(output, target)\n",
    "\n",
    "            # self.writer.set_step((epoch - 1) * len(self.valid_data_loader) + batch_idx, 'valid')\n",
    "            # self.writer.add_scalar('loss', loss.item())\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "            total_val_metrics += eval_metrics(output, target, metrics)\n",
    "\n",
    "            #self.writer.add_image('input', make_grid(data.cpu(), nrow=8, normalize=True))\n",
    "\n",
    "\n",
    "        # Add epoch metrics\n",
    "        val_loss = total_val_loss / len(test_dataloader)\n",
    "        val_metrics = (total_val_metrics / len(test_dataloader)).tolist()\n",
    "        \n",
    "        writer.add_scalar('loss', val_loss)\n",
    "        for i, metric in enumerate(metrics):\n",
    "            writer.add_scalar(\"%s\"%metric.__name__, val_metrics[i])\n",
    "\n",
    "    model.train()\n",
    "    return {\n",
    "        'val_loss': val_loss,\n",
    "        'val_metrics':val_metrics\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def3a631e1334f83a4f1a20fd1ff80db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "{'loss': 0.9813162775039673, 'metrics': [0.542875]}\n",
      "test\n",
      "{'val_loss': 1.0303447898477316, 'val_metrics': [0.5146484375]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c578c73c4f134c61bac412300bd4fd7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "{'loss': 0.8658858628273011, 'metrics': [0.601875]}\n",
      "test\n",
      "{'val_loss': 0.9524890128523111, 'val_metrics': [0.56689453125]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e87ded9e32c4a699a336bde11efe749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 150  # suggest training between 20-50 epochs\n",
    "\n",
    "start_time = datetime.datetime.now().strftime('%m%d_%H%M%S')\n",
    "checkpoint_dir = os.path.join(\"saved_cv\", start_time, 'checkpoints')\n",
    "log_dir = os.path.join(\"saved_cv\", start_time, 'logs')\n",
    "\n",
    "logger = logging.getLogger(\"SuperLogger\")\n",
    "\n",
    "\n",
    "writer = WriterTensorboardX(log_dir, logger, True)\n",
    "\n",
    "# Save configuration file into checkpoint directory:\n",
    "mkdir_p(checkpoint_dir)\n",
    "\n",
    "mnt_mode, mnt_metric = \"min\", \"val_loss\"\n",
    "\n",
    "mnt_best = math.inf if mnt_mode == 'min' else -math.inf\n",
    "early_stop = 40\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.train() # prep model for training\n",
    "\n",
    "metrics = [accuracy]\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    total_metrics = np.zeros(len(metrics))\n",
    "    writer.set_step(epoch) \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    _trange = tqdm(dataloader, leave=True, desc='')\n",
    "\n",
    "    for batch_idx, batch in enumerate(_trange):\n",
    "        batch = [b.to(\"cuda\") for b in batch]\n",
    "        data, target = batch[:-1], batch[-1]\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        #print(output, target)\n",
    "        # calculate the loss\n",
    "        loss = loss_fn.forward(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        #import pdb; pdb.set_trace()\n",
    "        train_loss += loss.item()\n",
    "        total_metrics += eval_metrics(output, target, metrics)\n",
    "        \n",
    "        if batch_idx % int(np.sqrt(dataloader.batch_size)) == 0:                \n",
    "                _str = 'Train Epoch: {} Loss: {:.6f}'.format(epoch,loss.item()) \n",
    "                _trange.set_description(_str)\n",
    "        \n",
    "    # print training statistics \n",
    "    # calculate average loss over an epoch\n",
    "    # Add epoch metrics\n",
    "    loss = train_loss / len(dataloader)\n",
    "    metric_epoch = (total_metrics / len(dataloader)).tolist()\n",
    "\n",
    "    writer.add_scalar('loss', loss)\n",
    "    for i, metric in enumerate(metrics):\n",
    "        writer.add_scalar(\"%s\"%metric.__name__, metric_epoch[i])\n",
    "\n",
    "    log = {\n",
    "        'loss': loss,\n",
    "        'metrics': metric_epoch\n",
    "    }\n",
    "    \n",
    "    print('train')\n",
    "    print(log)\n",
    "    print('test')\n",
    "    \n",
    "    val_log = valid_epoch(epoch)\n",
    "    print(val_log)\n",
    "    log = {**log, **val_log}\n",
    "\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "    \n",
    "    c_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "    \n",
    "    tot_log = {'epoch': epoch}\n",
    "    for key, value in log.items():\n",
    "        if key == 'metrics':\n",
    "            tot_log.update({mtr.__name__ : value[i] for i, mtr in enumerate(metrics)})\n",
    "        elif key == 'val_metrics':\n",
    "            tot_log.update({'val_' + mtr.__name__ : value[i] for i, mtr in enumerate(metrics)})\n",
    "        else:\n",
    "            tot_log[key] = value\n",
    "            \n",
    "    writer.add_scalar('lr', c_lr)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    best = False\n",
    "    try:\n",
    "        # check whether model performance improved or not, according to specified metric(mnt_metric)\n",
    "        improved = (mnt_mode == 'min' and tot_log[mnt_metric] < mnt_best) or \\\n",
    "                   (mnt_mode == 'max' and tot_log[mnt_metric] > mnt_best)\n",
    "    except KeyError:\n",
    "        logger.warning(\"Warning: Metric '{}' is not found. Model performance monitoring is disabled.\".format(mnt_metric))\n",
    "        mnt_mode = 'off'\n",
    "        improved = False\n",
    "        not_improved_count = 0\n",
    "\n",
    "    if improved:\n",
    "        mnt_best = tot_log[mnt_metric]\n",
    "        not_improved_count = 0\n",
    "        best = True\n",
    "    else:\n",
    "        not_improved_count += 1\n",
    "\n",
    "    if not_improved_count > early_stop:\n",
    "        logger.info(\"Validation performance didn\\'t improve for {} epochs. Training stops.\".format(early_stop))\n",
    "        break\n",
    "\n",
    "    if len(writer) > 0:\n",
    "        logger.info(\n",
    "            '\\nRun TensorboardX:\\ntensorboard --logdir={}\\n'.format(log_dir))\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        save_checkpoint(epoch, save_best=best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss_fn,\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            }, 'saved_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = next(iter(dataloader))\n",
    "tt = [t.to(\"cuda\") for t in tt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = tt[:-1], tt[-1]\n",
    "data = data if len(data) > 1 else data[0] \n",
    "output = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, targe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the NN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_features, n_hid, dropout):\n",
    "        super(Net, self).__init__()\n",
    "        #self.ML = Melspectrogram(fft_length = 2048, sample_rate=22050)\n",
    "        self.layers = nn.Sequential(\n",
    "            #nn.Dropout(dropout),\n",
    "            nn.Linear(n_features, 128),\n",
    "            nn.Tanh(),\n",
    "            #nn.BatchNorm1d(n_hid),\n",
    "            #nn.Dropout(dropout),            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.Tanh(),\n",
    "            #nn.BatchNorm1d(n_hid // 4),\n",
    "            #nn.Dropout(dropout),\n",
    "            nn.Linear(64, 4),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        #x = self.ML(x)\n",
    "        #print(x.shape)\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.contiguous().view(batch_size,-1)\n",
    "        #print(x.shape)\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "      \n",
    "# initialize the NN\n",
    "model = Net(5632, 250, 0.1).cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 3  # suggest training between 20-50 epochs\n",
    "\n",
    "model.train() # prep model for training\n",
    "\n",
    "lambda1 = 0.07\n",
    "lambda2 = 0.9\n",
    "alpha1 = 0.0001\n",
    "alpha2 = 1\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data, target in dataloader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        #print(output, target)\n",
    "        # calculate the loss\n",
    "        loss = loss_fn.forward(output, target-1)\n",
    "        \n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        #import pdb; pdb.set_trace()\n",
    "        train_loss += loss.item()\n",
    "        print(train_loss)\n",
    "        \n",
    "    # print training statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(dataloader.dataset)\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
